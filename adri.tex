Supongamos que tenemos un sistema de restricciones sobre las posibles
secuencias del tipo que puede ser representada por un gr\'afico lineal
como la Figura \ref{fig2}. Si las probabilidades de $p^{(s)}_{ij}$
fueron asignados a las distintas l\'ineas de conex\'ion del estado i
al estado j esto se convertir\'ia en una fuente.  Existe un trabajo en
particular que maximiza la entrop\'ia resultante (v\'ease Ap\'endice
\ref{a4}).

\begin{theorem}
El sistema de restricciones consideradas como un canal tiene una
capacidad $C=\log W$.  Si nosotros asignamos
\begin{equation}
p^{(s)}_{ij}=\frac{B_{j}}{B_{i}}W^{-l^{(s)}_{ij}}
\end{equation}
donde $l^{(s)_{ij}}$ es la duraci\'on de $s$\'{e}simo s\'imbolo que va
desde el estado $i$ al estado $j$ y satisface $B_{i}$
\begin{equation}
B_{i}=\sum_{s,j} B_{j}W^{-l^{(s)}_{ij}}
\end{equation}
entonces $H$ es maximizada e igual a $C$.
\end{theorem}

Por asignaci\'on adecuada de las probabilidades de transici\'on de la
entrop\'ia de los s\'imbolos en un canal puede ser maximizada a la
capacidad del canal.

\chapter{El teorema fundamental para un canal sin ruido}

Ahora vamos a justificar nuestra interpretaci\'on de $H$, tal como la
tasa de generaci\'on de informaci\'on por demostrar que $H$ determina la
capacidad de canal requerida con la codificaci\'on m\'as eficiente.

\begin{theorem}
Vamos a tener una fuente de entrop\'ia H (bits por s\'imbolo) y un
canal con una capacidad $C$ (bits por segundo). Entonces es posible
codificar la salida de la fuente de tal manera que se transmita a la
media $\frac{C}{H}-\epsilon$ s\'{\i}mbolos por segundo, sobre el canal
donde es arbitrariamente peque\~{n}o. No es posible transmitir a una
tasa promedio mayor que $\frac{C}{H}$.
\label{t9}
\end{theorem}

La parte inversa del teorema, que $\frac{C}{H}$ no puede ser excedido,
puede probarse por senalar que la entrop\'ia de la entrada del canal
por segundo es igual a la de la fuente, ya que el transmisor debe ser
no singular, y tambi\'en esta entrop\'ia no puede exceder la capacidad
del canal. Por lo tanto $H' \leq C$ y el n\'umero de
s\'imbolos por segundo = $H'/ H \leq C/H$. 

La primera parte del teorema se demostr\'o de dos maneras diferentes.
El primer m\'etodo es considerar el conjunto de todas las secuencias
de $N$ s\'imbolos producidos por la fuente. Para $N$ grandes podemos
dividir en dos grupos, uno que contiene menos de $2^{(H+\eta)^{N}}$
miembros $N$ y la segunda contiene menos de $2^{RN}$ miembros (donde
$R$ es el logaritmo del n\'umero de s\'imbolos diferentes) y que tiene
una probabilidad total menos que $\mu$. A medida que aumenta $N$,
$\eta$ y $\mu$ se aproximan a cero. El n\'umero de se\~{n}ales de
duraci\'on $T$ en el canal es mayor que $2^{(C-\theta)T}$ con $\theta$
peque\~{n}a cuando $T$ es grande. Si nosotros elegimos
\begin{equation}
T =(\frac{H}{C}+\lambda)N
\end{equation}
entonces no habra un numero suficiente de secuencias de simbolos de
canal para el grupo de alta probabilidad cuando $N$ y $T$ son lo
suficientemente grandes (por peque\~{n}o $\lambda$) y tambi\'en
algunos adicionales. El grupo de alta probabilidad se codifica en
forma arbitraria de uno a una direcci\'on dentro de este conjunto. Las
restantes secuencias est\'an representadas por secuencias no
utilizadas para el grupo de alta probabilidad. Esta secuencia especial
act\'ua como una se\~{n}al de inicio y parada para un c\'odigo
diferente. En el medio se permite un tiempo suficiente para dar
suficientes secuencias diferentes a todos los mensajes de baja
probabilidad. Esto requerir\'a
\begin{equation}
T_{1} = \left(\frac{R}{C} + \varphi\right)N
\end{equation}
cuando $\varphi$ es peque\~{n}o. La tasa media de transmisi\'on de s\'imbolos de mensajes por segundo ser\'a entonces 
mayor que
\begin{equation}
\left[(1-\delta)\frac{T}{N} + \delta\frac{T_{1}}{N}\right]^{-1} 
= 
\left[(1-\delta) 
\left(\frac{H}{C} + \lambda \right) + 
\delta \left(\frac{R}{C} + \varphi \right) \right]^{-1}.
\end{equation}

Como $N$ incrementa $\delta$, $\lambda$ y $\varphi$ se aproximan a cero y la tasa se aproxima a  $\frac{C}{H}$.
Otro m\'etodo de realizar esta codificaci\'on y demostrando as\'i el teorema se puede describir de la siguiente manera:\\
Organizar los mnsajes de longitud $N$ en orden decreciente de probabilidad y suponiendo que sus probabilidades 
son $p_{1}\geq p_{2}\geq p_{3} \ldots \geq p_{n}$.

Vamos a $P_{s}=\sum^{s-1}_{1} pi$; que es $P_{s}$ es la probabilidad
acumulada hasta, pero no incluyendo, $P_{s}$.\\ En primer lugar,
codificar en un sistema binario. El c\'odigo binario para el mensaje
de $s$ se obtiene mediante la expansi\'on de $P_{s}$ como un n\'umero
binario. La expansi\'on se lleva a cabo para $m_{s}$ lugares, donde
$m_{s}$ es el n\'umero entero que satisface:
\begin{equation}
\log_{2} \frac{1}{P_{s}}\leq m_{s} < 1 + \log_{2}\frac{1}{P_{s}} .
\end{equation} 
Por lo tanto los mensajes de alta probabilidad son representados por
los c\'odigos de acceso y los de baja probabilidad de c\'odigos
largos. A partir de estas desigualdades que tenemos:
\begin{equation}
\frac{1}{2^{m_{s}}} \leq P_{s} < \frac{1}{2^{m_{s} -1}}.
\end{equation}
El c\'odigo por $P_{s}$ ser\'a diferente de todos los subsiguientes en
uno o m\'as de sus lugares $m_{s}$, ya que todos los restantes $P_{i}$
es al menos $\frac{1}{2^{m_{s}}}$ grande y por lo tanto sus
expansiones binarios difieren en los primeros lugares de $m_{s}$. En
consecuencia, todos los c\'odigos son diferentes, y es posible
recuperar el mensaje de su c\'odigo. Si las secuencias de los canales
no son ya secuencias de d\'igitos binarios, que se pueden atribuir
n\'umeros binarios de manera arbitraria y por lo tanto el c\'odigo
binario traducido en se\~{n}ales adecuadas para el canal.

El n\'umero promedio de $H'$ de d\'igitos binarios utilizados
por \'imbolo de mensaje original se calcula f\'acilmente. Nosotros
tenemos
\begin{equation}
H' = \frac{1}{N} \sum m_{s} p_{s}.
\end{equation}
Pero
\begin{equation}
\frac{1}{N} \sum 
\left(\log_{2} \frac{1}{p_{s}} \right) p_{s} 
\leq \frac{1}{N} \sum \left( 1+\log_{2} \frac{1}{p_{s}} \right) p_{s},
\end{equation}
y por lo tanto,
\begin{equation}
G_{N} \leq H' < G_{N} + \frac{1}{N}.
\end{equation}

Como $N$ aumenta $G_{N}$ se aproxima a $H$, la entrop\'ia de la fuente
y $H'$ se acerca $H$. Vemos de esto que la ineficiencia en la
codificaci\'on, cuando se utiliza s\'olo un retardo finito de $N$
s\'imbolos, no tiene que ser mayor que $N$ m\'as la diferencia entre
la verdadera entrop\'ia de $H$ y la entrop\'ia $G_{N}$ calculada para
secuencias de longitud $N$. El exceso de tiempo por ciento necesario
sobre el ideal es por lo tanto, menos
\begin{equation}
\frac{G_{N}}{H} + \frac{1}{HN} - 1.
\end{equation}

Este m\'etodo de codificaci\'on es sustancialmente el mismo que uno
que encontraron de forma independiente por \citet{fano}. Su m\'etodo
es el de organizar los mensajes de longitud $N$ en orden decreciente
de probabilidad. Dividir esta serie en dos grupos de tan iguales
probabilidades sea posible. Si el mensaje est\'a en el primer grupo su
primer d\'igito binario ser\'a 0, en caso contrario 1. Los grupos
est\'an divididos de manera similar en subconjuntos de casi la misma
probabilidad y el subgrupo d\'igitos determina el segundo d\'igito
binario. Este proceso contin\'ua hasta que cada subconjunto contiene
s\'olo un mensaje. Es f\'acil ver que, aparte de peque\~{n}as
diferencias (generalmente en el \'ultimo d\'igito) esto equivale a lo
mismo que el proceso de c\'alculo descrito anteriormente.

\chapter{Discusi\'on y ejemplos}

Con el fin de obtener la m\'axima transferencia de energ\'ia de un
generador a una carga, un transformador debe en general ser
introducido de modo que el generador como se ve desde la carga tiene
la resistencia de carga. La situaci\'on aqu\'i es m\'as o menos
similar. El transductor hace que la codificaci\'on debe coincidir con
la fuente para el canal en un sentido estad\'istico. La fuente como se
ve desde el canal a trav\'es del transductor debe tener la misma
estructura estad\'istica como la fuente que maximiza la entrop\'ia en
el canal. El contenido del teorema \ref{t9} es que, a pesar de una
coincidencia exacta no es posible en general, podemos aproximar la
mayor fidelidad si lo deseas. La relaci\'on de la velocidad real de
transmisi\'on de la capacidad $C$ se puede llamar la eficiencia del
sistema de codificaci\'on. Esto es, por supuesto, igual a la
relaci\'on de la entrop\'ia real de los s\'imbolos de canal a la
entrop\'ia m\'axima posible. En general, la codificaci\'on ideal o
casi ideal requiere un largo retraso en el transmisor y el receptor.
En el caso silenciosos que hemos estado considerando, la funci\'on
principal de este retraso es permitir razonablemente buena
adaptaci\'on de las probabilidades correspondientes a longitudes de
secuencias. Con un buen c\'odigo el logaritmo de la probabilidad
rec\'iproco de un mensaje largo debe ser proporcional a la duraci\'on
de la se\~{n}al correspondiente, de hecho
\begin{equation}
\left |\frac{\log p^{-1}}{T} - C \right |
\end{equation}
debe ser peque\~{n}a para todos, pero una peque\~{n}a fracci\'on de
los mensajes largos. Si una fuente puede producir s\'olo un mensaje
particular, su entrop\'ia es cero, y no se requiere ning\'un canal.
Por ejemplo, una m\'aquina de computaci\'on configurado para calcular
los d\'igitos sucesivos de produce una secuencia definida con ning\'un
elemento de azar. No se requiere ning\'un canal de "transmitir" este a
otro punto. Se podr\'ia construir una segunda m\'aquina para calcular
la misma secuencia en el punto. Sin embargo, esto puede ser poco
pr\'actico. En tal caso se puede optar por ignorar algunos o todos de
los conocimientos estad\'isticos que tenemos de la fuente. Podr\'iamos
considerar los d\'igitos de $\pi$ son una secuencia aleatoria en que
se construye un sistema capaz de enviar cualquier secuencia de
d\'igitos.

De manera similar, podemos optar por utilizar algunos de nuestro
conocimiento estad\'istico de Ingl\'es en la construcci\'on de un
c\'odigo, pero no todos de la misma. En este caso se considera la
fuente de la m\'axima entrop\'ia sujeta a las condiciones
estad\'isticas que deseamos conservar. La entrop\'ia de esta fuente
determina la capacidad del canal que es necesaria y suficiente. En
$\pi$ el ejemplo de la \'unica informaci\'on retenida es que todos los
d\'igitos se eligen desde el conjunto $0, 1, \ldots, 9$. En el caso de
Ingl\'es que uno podr\'ia desear utilizar el ahorro estad\'istica
posible debido a las frecuencias de la letra, pero nada m\'as. La
fuente de entrop\'ia m\'axima es entonces la primera aproximaci\'on al
Ingl\'es y su entrop\'ia determina la capacidad del canal deseado.

Como un simple ejemplo de algunos de estos resultados consideran una
fuente que produce una secuencia de letras elegidas de entre $A$, $B$,
$C$, $D$ con probabilidades $\frac{1}{2}$, $\frac{1}{4}$,
$\frac{1}{8}$, $\frac{1}{8}$, s\'imbolos sucesivos se eligen
independientemente. Nosotros tenemos
\begin{equation}
H \quad = \quad -\left(\frac{1}{2} \log \frac{1}{2} + \frac{1}{4} \log \frac{1}{4} + \frac{2}{8} \log \frac{1}{8}\right) \\
\quad = \quad \frac{7}{4}
\end{equation}
bits por s\'imbolo.

Por lo tanto se puede establecer un sistema de codificaci\'on para
codificar los mensajes aproximada de esta fuente en d\'igitos binarios
con un promedio de $\frac{7}{4}$ d\'igito binario por s\'imbolo. En
este caso podemos realmente alcanzar el valor l\'imite por el
siguiente c\'odigo (obtenida por el m\'etodo de la segunda prueba del
teorema \ref{t9}):

\begin{tabular}{ll}
 $A$ & 0 \\
 $B$ & 10\\
 $C$ & 110\\
 $D$ & 111
\end{tabular}

El n\'umero medio de d\'igitos binarios utilizados en la codificaci\'on de una secuencia de $N$ s\'imbolos es:
\begin{equation}
N \left(\frac{1}{2} x_1 + \frac{1}{4} x_2 + \frac{2}{8} x_3\right) 
= \frac{7}{4}N.
\end{equation}
Es f\'acil ver que los d\'igitos binarios $0$, $1$ tienen
probabilidades de $\frac{1}{2}$, $\frac{1}{2}$ por lo que la $H$ para
las secuencias codificadas es un bit por s\'imbolo. Puesto que, en
promedio, tenemos $\frac{7}{4}$ s\'imbolos binarios por carta
original, las entrop\'ias sobre una base de tiempo son los mismos. La
entrop\'ia m\'axima posible para el conjunto original es $\log_2 4 =
2$, que se producen cuando $A$, $B$, $C$, $D$ tienen probabilidades de
$\frac{1}{4}$, $\frac{1}{4}$, $\frac{1}{4}$, $\frac{1}{4}$. Por lo
tanto, la entrop\'ia relativa es $\frac{7}{8}$. Podemos traducir las
secuencias binarias en el conjunto original de los s\'imbolos en un
dos-a-uno en la siguiente tabla:

\begin{tabular}{ll}
00  &	$A'$\\
01	&	$B'$\\
10	&	$C'$\\
11	&	$D'$\\
\end{tabular}

