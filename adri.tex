Supongamos que tenemos un sistema de restricciones sobre las posibles
secuencias del tipo que puede ser representada por un gr\'afico lineal
como la Figura \ref{fig2}. Si las probabilidades de $p^{(s)}_{ij}$
fueron asignados a las distintas l\'ineas de conex\'ion del estado i
al estado j esto se convertir\'ia en una fuente.  Existe un trabajo en
particular que maximiza la entrop\'ia resultante (v\'ease Ap\'endice
\ref{a4}).

\begin{theorem}
El sistema de restricciones consideradas como un canal tiene una
capacidad $C=\log W$.  Si nosotros asignamos
\begin{equation}
p^{(s)}_{ij}=\frac{B_{j}}{B_{i}}W^{-l^{(s)}_{ij}}
\end{equation}
donde $l^{(s)_{ij}}$ es la duraci\'on de $s$\'{e}simo s\'imbolo que va
desde el estado $i$ al estado $j$ y satisface $B_{i}$
\begin{equation}
B_{i}=\sum_{s,j} B_{j}W^{-l^{(s)}_{ij}}
\end{equation}
entonces $H$ es maximizada e igual a $C$.
\end{theorem}

Por asignaci\'on adecuada de las probabilidades de transici\'on de la
entrop\'ia de los s\'imbolos en un canal puede ser maximizada a la
capacidad del canal.

\section{El teorema fundamental para un canal sin ruido}

Ahora vamos a justificar nuestra interpretaci\'on de H, tal como la tasa de generaci\'on de informaci\'on por 
demostrar que H determina la capacidad de canal requerida con la codificaci\'on m\'as eficiente.
Teorema 9: Vamos a tener una fuente de entrop\'ia H (bits por s\'imbolo) y un canal con una capacidad C(bits por 
segundo). Entonces es posible codificar la salida de la fuente de tal manera que se transmita a la media 
\frac{C}{H}-\epsilon simbolos por segundo, sobre el canal donde es arbitrariamente peque\~{n}o. No es posible 
transmitir a una tasa promedio mayor que \frac{C}{H}.\\


La parte inversa del teorema, que \frac{C}{H} no puede ser excedido, puede probarse por senalar que la entrop\'ia 
de la entrada del canal por segundo es igual a la de la fuente, ya que el transmisor debe ser no singular, y tambi\'en 
esta entrop\'ia no puede exceder la capacidad del canal. Por lo tanto \textit{H'} \leq \textit{C} y el n\'umero de 
s\'imbolos por segundo =\textit{H'/ H}\leq \textit{C/H}\\
La primera parte del teorema se demostr\'o de dos maneras diferentes. El primer m\'etodo es considerar el conjunto 
de todas las secuencias de \textit{N} s\'imbolos producidos por la fuente. Para \textit{N} grandes podemos dividir 
en dos grupos, uno que contiene menos de 2^{(H+\eta)^{N}} miembros N y la segunda contiene menos de 2^{RN} miembros 
(donde R es el logaritmo del n\'umero de s\'imbolos diferentes) y que tiene una probabilidad total menos que \mu. 
A medida que aumenta N, \eta y \mu se aproximan a cero. El n\'umero de se\~{n}ales de dueraci\'on \textit{T} en el 
canal es mayor que \textit{2^{(C-\theta)T}} con \theta peque\~{n}a cuando \textit{T} es grande. Si nosotros elegimos:\\
\begin{center}
\textit{T =(\frac{H}{C}+\lambda)N}
\end{center}
entonces no habra un numero suficiente de secuencias de simbolos de canal para el grupo de alta probabilidad cuando 
\textit{N} y \textit{T} son lo suficientemente grandes(por peque\~{n}o \lambda) y tambi\'en algunos adicionales. El 
grupo de alta probabilidad se codifica en forma arbitraria de uno a una direcci\'on dentro de este conjunto. Las 
restantes secuencias est\'an representadas por secuencias no utilizadas para el grupo de alta probabilidad. Esta 
secuencia especial act\'ua como una se\~{n}al de inicio y parada para un c\'odigo diferente. En el medio se permite 
un tiempo suficiente para dar suficientes secuencias diferentes a todos los mensajes de baja probabilidad. Esto 
requerir\'a
\begin{center}
\textit{T_{1} = (\frac{R}{C} + \varphi)N}
\end{center}
cuando \varphi es peque\~{n}o. La tasa media de transmisi\'on de s\'imbolos de mensajes por segundo ser\'a entonces 
mayor que
\begin{flushleft}
[(1-\delta)\frac{T}{N} + \delta\frac{T_{1}}{N}]^{-1} = [(1-\delta) (\frac{H}{C} + \lambda) + \delta(\frac{R}{C} + \varphi)]^{-1}
\end{flushleft}

Como $N$ incrementa \delta, \lambda y \varphi se aproximan a cero y la tasa se aproxima a  $\frac{C}{H}$.
Otro m\'etodo de realizar esta codificaci\'on y demostrando as\'i el teorema se puede describir de la siguiente manera:\\
Organizar los mnsajes de longitud $N$ en orden decreciente de probabilidad y suponiendo que sus probabilidades 
son \textit{p_{1}\geq p_{2}\geq p_{3} ... \geq p_{n}}.

Vamos a $P_{s}=\sum^{s-1}_{1} pi$; que es $P_{s}$ es la probabilidad acumulada hasta, pero no incluyendo, $P_{s}$.\\
En primer lugar, codificar en un sistema binario. El c\'odigo binario para el mensaje de $s$ se obtiene mediante la 
expansi\'on de $P_{s}$ como un n\'umero binario.
La expansi\'on se lleva a cabo para $m_{s}$ lugares, donde $m_{s}$ es el n\'umero entero que satisface:
\begin{equation}
log_{2} \frac{1}{$P_{s}$}\leq $m_{s}$ \bigodot 1 + log_{2}\frac{1}{$P_{s}$} .
\end{equation} \\

Por lo tanto los mensajes de alta probabilidad son representados por los c\'odigos de acceso y los de baja 
probabilidad de c\'odigos largos. A partir de estas desigualdades que tenemos:
\begin{equation}
\frac{1}{2^{$m_{s}$}} \leq $P_{s}$ < \frac{1}{2^{$m_{s}$ -1}}.
\end{equation}\\
El c\'odigo por $P_{s}$ ser\'a diferente de todos los subsiguientes en uno o m\'as de sus lugares $m_{s}$, ya que 
todos los restantes $P_{i}$ es al menos \frac{1}{2^{$m_{s}$}} grande y por lo tanto sus expansiones binarios difieren 
en los primeros lugares de $m_{s}$. En consecuencia, todos los c\'odigos son diferentes, y es posible recuperar el 
mensaje de su c\'odigo. Si las secuencias de los canales no son ya secuencias de d\'igitos binarios, que se pueden 
atribuir n\'umeros binarios de manera arbitraria y por lo tanto el c\'odigo binario traducido en se\~{n}ales 
adecuadas para el canal.\\

El n\'umero promedio de $H'$ de d\'igitos binarios utilizados por \'imbolo de mensaje original se calcula 
f\'acilmente. Nosotros tenemos
\begin{equation}
$H'$ = \frac{1}{$N$} \sum $m_{s} p_{s}$
\end{equation}
pero,\\
\begin{equation}
\frac{1}{$N$} \sum (log_{2} \frac{1}{$p_{s}$})$p_{s}$ \leq \frac{1}{$N$} \sum (1+log_{2} \frac{1}{$p_{s}$}) $p_{s}$
\end{equation}
y por lo tanto,\\
\begin{equation}
$G_{N}$ \leq $H'$ < $G_{N}$ + \frac{1}{$N$}
\end{equation}

Como $N$ aumenta $G_{N}$ se aproxima a $H$, la entrop\'ia de la fuente y $H'$ se acerca $H$.
Vemos de esto que la ineficiencia en la codificaci\'on, cuando se utiliza s\'olo un retardo finito de $N$ s\'imbolos,
no tiene que ser mayor que $N$ m\'as la diferencia entre la verdadera entrop\'ia de $H$ y la entrop\'ia $G_{N}$ calculada para secuencias de longitud $N$.
El exceso de tiempo por ciento necesario sobre el ideal es por lo tanto, menos

\begin{equation}
\frac{$G_{N}$}{$H$} + \frac{1}{$HN$} - 1.
\end{equation}\\

