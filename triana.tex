{\textexclamdown}Hola, mundo!

\section{Entrop\'ia de la suma de dos conjuntos}

	Si tenemos dos conjuntos de funciones $f_{\alpha}(t)$ y
	$g_{\beta}(t)$ podemos formar un nuevo conjunto por
	"adici\'on". Supongamos que el primer conjunto tiene la
	funci\'on de densidad de probabilidad $p(x_{1},...,x_{n})$ y
	el segundo $q(x_{1},...,x_{n})$.  Despu\'es la funci\'on de
	densidad para la adici\'on es dada por la convoluci\'on
	
	\begin{equation} r(x_{1},...,x_{n}) = \int \cdots \int
		p(y_{1},...,y{n})q(x_{1}-y_{1},...,x_{n}-y_{n})dy_{1}...dy_{n} \end{equation}
	
	F\'isicamente esto corresponde a sumar los ruidos o se\~nales
	representados por los conjuntos originales de las funciones
	
		
	El siguiente resultado es derivado en el Ap\'endice 6
	
    \begin{theorem} Deje que la potencia media de los 2 conjuntos sea
    $N_{1}$ y $N_{2}$ y deje que sus poderes de entrop\'ia sean
    $\bar{N}_1$ y $\bar{N}_2$ . Entonces, el poder de entrop\'ia de
    la suma, $N_{3}$, est\'a delimitado
    por \begin{equation} \bar{N}_1+\bar{N}_2 \le \bar{N}_3 \le
    N_{1}+N_{2}.  \end{equation} \end{theorem}
	
	El ruido blanco Gaussiano tiene la peculiar propiedad que
	puede absorber cualquier otro ruido o conjunto de se\~nales
	que puede ser a\~nadido a la misma con una potencia de
	entrop\'ia resultante aproxim\'adamente igual a la suma de a
	potencia ruido blanco y la potencia de la se\~nal(medida
	apartir del valor promedio de la se\~nal, que es normalmente
	cero), siempre que la potencia de la señal es peque\~na, en
	cierto sentido, en comparaci\'on con el ruido.
	
	Considere el espacio de la funci\'on asociada con estos
	conjuntos que tienen dimensiones n. El ruido blanco
	corresponde a la distribuci\'on Gaussiana esf\'erica en este
	espacio. El conjunto de la se\~nal corresponde a otra
	distribuci\'on de probabilidad, no necesariamente Gaussiana o
	esf\'erica. Deje que los segundos momentos de esta
	distribuci\'on alrededor de su centro de gravedad sea
	$a_{ij}$.  Es decir, si p($x_{1},...,x_{n}$) es la funci\'on
	de distribuci\'on de densidad \begin{equation}
	a_{ij}=\int \cdots \int
	p(x_{i}-\alpha_{i})(x_{j}-\alpha_{j}dx_{1} \cdots
	dx_{n}) \end{equation} donde $\alpha_{i}$ son las coordenadas
	del centro de gravedad. Ahora $a_{ij}$ es una forma
	cuadr\'atica positiva, y podemos rotar nuestro sistema de
	coordenadas para alinearla con las direcciones principales de
	esta forma. $a_{ij}$ es entonces reducido a la forma diagonal
	$b_{ii}$. Se requiere que cada $b{ii}$ sea peque\~no comparado
	con N, el cuadrado del radio de la distribuci\'on esf\'erica.
	
	En este caso, la convoluci\'on del ruido y se\~nal producen
	aproximadamente una distribuci\'on Gaussiana cuya forma
	cuadr\'atica correspondiente es \begin{equation} N +
	b_{ii}.  \end{equation}
	
	El potencial de entrop\'ia de esta distribuci\'on
	es \begin{equation} [\Pi(N +
	b_{ii})]^\frac{1}{n} \end{equation} o
	aproximadamente \begin{equation} = [(N)^n + \Sigma
	b_{ii}(N)^n-1]^\frac{1}{n} \end{equation} \begin{equation} = N
	+ \frac{1}{n}\Sigma b_{ii}.  \end{equation} El \'ultimo
	t\'ermino es la potencia de la se\~nal, mientras que el
	primero es la potencia del ruido.
	
	
	\chapter{El canal continuo}
	
\section{La capacidad de un canal continuo}	
	
	En un canal continuo de las se\~nales de entrada o
	transmitidas ser\'an funciones continuas de tiempo $f(t)$
	pertenecientes a un determinado conjunto, y la señales de
	salida o recibidas ser\'an versiones perturbadas de
	estas. Vamos a considerar solo el caso en que ambas se\~nales
	transmitidas y recibidas se limitan a una determinada banda
	$W$. Pueden ser despu\'es identificadas por un tiempo $T$, por
	los n\'umeros $2TW$, y su estructura estad\'istica de las
	funciones de distribuci\'on finitos tridimensionales. As\'i
	las estad\'isticas de la se\~nal transmitida ser\'a
	determinada por
	\begin{equation}
		P(x_{1}, \cdots ,x_{n}) = P(x)
	\end{equation}	 
	y los del ruido por la distribuci\'on de probabilidad condicional
	\begin{equation}
		P_{x1, \cdots ,xn} (y_{1}, \cdots ,y_{n}) = P_{x}(y).
	\end{equation}
	
	La tasa de transmisi\'on de informaci\'on por un canal
	continuo se define de una manera an\'aloga a la de un canal
	separado, esto es \begin{equation} R =
	H(x)-H_{y}(x), \end{equation} donde $H(x)$ es la entrop\'ia de
	la entrada y $H_{y}(x)$ el equ\'ivoco. La capacidad del canal
	$C$ se define como el m\'aximo de R cuando var\'ian la entrada
	para todos los conjuntos posibles. Esto significa que en una
	aproximaci\'on dimensional finita debemos variar $P(x) =
	P(x_{1},...,x_{n})$ y maximizar \begin{equation} - \int
	P(x) \log P(x)dx + \int\int
	P(x,y)\log \frac{P(x,y)}{P(y)}dxdy.  \end{equation} Esto puede
	ser escrito \begin{equation} \int\int
	P(x,y) \log \frac{P(x,y)}{P(x)P(y)}dxdy \end{equation} usando
	el hecho que $\int\int P(x,y)\log P(x)dxdy = \int P(x)\log
	P(x)dx$. La capacidad del canal se expresa as\'i:
	\begin{equation}
		C = \limsup_{T\to \infty P(x)} \frac{1}{T}\int\int P(x,y)\log \frac{P(x,y)}{P(x)~
		P(y)}dxdy.
	\end{equation}
	
	Es obvio que en esta forma $R$ y $C$ son independientes del
	sistema de coordenadas dado que el numerador y denominador en
	$\log\frac{P(x,y)}{P(x)P(y)}$ ser\'a multiplicado por los
	mismos factores cuando $x$ y $y$ son transformados en
	cualquier forma uno a uno. Esta expresi\'on integral para $C$
	es m\'{a}s general que $H(x)-H_{y}(x)$. Correctamente
	interpretada (ver Anexo 7) que siempre existir\'a mientras
	$H(x)-H_{y}(x)$ puede asumir una forma indeterminada $\infty
	- \infty$ en algunos casos. Esto ocurre, por ejemplo, si $x$
	est\'a limitada a una superficie de menos dimensiones que $n$
	en su aproximaci\'on $n$ dimensional.
	
	Si la base logar\'itmica utilizada en computaci\'on $H(x)$ y
	$H_{y}(x)$ es dos entonces $C$ es el n\'umero m\'aximo de
	d\'igitos binarios que pueden ser enviados por segundo a
	trav\'es del canal con equivocaci\'on arbitrariamente
	peque\~na, al igual que en el caso discreto. Esto se puede ver
	f\'isicamente al dividir el espacio de se\~nales en un gran
	n\'umero de celdas peque\~nas y suficientemente peque\~no para
	que la densidad de probabilidad $P_{x}(y)$ de la se\~nal x que
	est\'{a}q siendo perturbado hasta el punto de que $y$ es
	sustancialmente constante en una celda (ya sea de $x$ o
	$y$). Si las celdas son consideradas como puntos distintos, la
	situaci\'on es esencialmente la misma que un canal discreto y
	las pruebas usadas se aplicar\'an all\'{a}. Pero est\'{a}
	claro que f\'isicamente esta cuantificaci\'on del volumen en
	puntos individuales no puede de ninguna manera pr\'actica
	alterar significativamente la respuesta final, siempre que las
	regiones sean suficientemente peque\~nas. As\'i la capacidad
	ser\'{a} el l\'imite de las capacidades de las subdivisiones
	discretas y esto es es s\'olo la capacidad continua definida
	anteriormente.
	
	En el lado matem\'atico se pede demostrar primero (ver el
	Ap\'endice 7) que si $u$ es el mensaje, $x$ es la se\~nal, $y$
	es la se\~nal recibida (perturbada por el ruido) y $v$ es el
	mensaje recuperado, entonces
	\begin{equation}
		H(x) - H_{y}(x) \le H(u) - H_{v}(u).
	\end{equation}	 
	
	Independientemente de lo que las operaciones realizan en $u$
	para obtener $x$ o en $y$ para obtener $v$. No importa como
	modificamos los d\'igitos binarios para obtener la se\~nal, o
	como decodificamos la se\~nal recibida para recuperar el
	mensaje, la tasa discreta para los d\'igitos binarios no
	excede la capacidad del canal que tenemos definida. Por otra
	parte, es posible bajo condiciones muy generales encontrar un
	sistema de codificaci\'on para transmitir d\'igitos binarios
	en la tasa $C$ con peque\~na equivocaci\'on o frecuencia de
	errores como se desee. Este es el caso, por ejemplo, si,
	cuando tomamos un espacio finito de aproximaci\'on para las
	funciones de las se\~nales, $P(x,y)$ es continuo tanto en $x$
	como en $y$, excepto en un conjunto de puntos de probabilidad
	cero.
	
	Un caso especial se produce cuando el ruido se a\~nade a la
	se\~nal y es independiente de ello (en el sentido de la
	probabilidad). Entonces $P_{x}(y)$ es una funci\'on s\'olo de
	la diferencia $n = (y-x)$,
	\begin{equation}
		P_{x}(y) = Q(y-x).
	\end{equation}
	
	%%%%%%%%%%%%% PAGINA 43 ES LA SIGUIENTE %%%%%%%%%%	
	
	y nosotros podemos asignar una entrop\'ia definida al ruido (independientemente de 
	las estad\'isticas de la se\~nal), es decir, la entrop\'ia de la distribuci\'on
	$Q(n)$. Esta entrop\'ia se denotar\'a por $H(n)$.
	
	\begin{em}
		Teorema 16: Si la se\~nal y el ruido son independientes y la se\~nal recibida
		es la suma de se\~nal transmitida y el ruido entonces la tasa de transmisi\'on es
	\end{em}
	
	\begin{equation}
		R = H(y) - H(n),
	\end{equation}
	
	es decir, la entrop\'ia de la se\~nal recibida menos la entrop\'ia del ruido. La
	capacidad del canal es
	
	\begin{equation}
		C = \limsup_{P(x)} H(y) - H(n)
	\end{equation}
	
	Tenemos, desde $y = x+n$
	
	\begin{equation}
		H(x,y) = H(x,n).
	\end{equation}
	
	Expandiendo el lado izquierdo y utilizando el hecho de que $x$ y $n$ son independientes
	
	\begin{equation}
		H(y) + H_{y}(x) = H(x) + H(n).
	\end{equation}
	
	Por lo tanto
	
	\begin{equation}
		R = H(x) - H_{y}(x) = H(y) - H(n).
	\end{equation}
	
	Puesto que $H(n)$ es independiente de $P(x)$, maximizando R requiere maximizar $H(y)$,
	la entrop\'ia de la se\~nal recibida. Si existen ciertas restricciones en el conjunto
	de las se\~nales transmitidas, la entrop\'ia de la se\~nal recibida debe ser maximizada
	sujeto a esas restricciones.
	
	\bigskip
	25. CAPACIDAD DE LA SE\~NAL  CON UNA LIMITACI\'ON DE POTENCIA MEDIA
	\bigskip	
	
	Una sencilla aplicaci\'on del teorema 16 es el caso cuando el ruido es un ruido
	t\'ermico blanco y las se\~nales transmitidas est\'an limitadas a un cierto 
	promedio de potencia $P$. Luego las se\~nales recibidas tienen una potencia media
	$P + N$ donde $N$ es la potencia media del ruido. La entrop\'ia m\'axima para las
	se\~nales recibidas se produce cuando tambi\'en forman un ruido blanco ya que 
	es la entrop\'ia mayor posible para una potencia $P + N$ y puede obtenerse mediante
	una elecci\'on adecuada de las se\~nales transmitidas, a saber, si forman un conjunto
	de ruido blanco de potencia $P$. La entrop\'ia (por segundo) del conjunto recibido es luego
	
	\begin{equation}
		H(y) = W\log 2\pi e(P+N),
	\end{equation}
	
	Y la entrop\'ia de ruido es
	
	\begin{equation}
		H(n) = W\log 2\pi eN.
	\end{equation}
	
	La capacidad del canal es
	
	\begin{equation}
		C = H(y) - H(n) = W\log \frac{P + N}{N}.
	\end{equation}
	
	Resumiendo tenemos lo siguiente:
	
	Teorema 17: La capacidad de un canal de banda $W$ de potencia perturbada por el ruido
	t\'ermico blanco $N$ cuando la potencia de transmisi\'on media se limita a $P$ viene
	dada por
	
	\begin{equation}
		C = W\log \frac{P + N}{N}.
	\end{equation}
	
	Esto significa que por sistemas de codificaci\'on suficientemente implicados se puede
	transmitir d\'igitos binarios a la tasa $W \log_{2}\frac{P + N}{N}$ bits por segundo,
	con arbitrariamente peque\~na frecuencia de errores. No es posible transmitir a una
	velocidad mayor por cualquier sistema de codificaci\'on sin una frecuencia
	definida positiva de errores.
	
	Para aproximar esta limitaci\'on de la tasa de transmisi\'on, las se\~nales transmitidas
	deben aproximarse, en propiedades estad\'isticas, un ruido blanco. Un sistema que se 
	aproxima a la tasa ideal puede ser descrito como sigue: Sea $M = 2^s$
	
	%%%%%%%%%%%%%% SIGUE LA PAGINA 44
	Meh meh
	
	%Ecuacion sin numero
	\begin{displaymath}
		x^{2}+y^{2}=h^2
	\end{displaymath}

	%Ecuacion
	\begin{equation}
		p\left ( x_{1},...,x_{n}\right )
	\end{equation}

	%Ecuacion
	\begin{equation}
		{H}'=-\lim_{n\rightarrow \infty }\frac{1}{n}\int \cdots \int p\left ( 							x_{1},...,x_{n}\right ) \log p\left ( x_{1},...,x_{n}\right )dx_{1}...dx_{n}
	\end{equation}

	%Ecuacion
	\begin{equation}
		{H}'=\log \sqrt{2\pi eN}
		\newline
		H=W\log 2\pi eN
	\end{equation}		
	



